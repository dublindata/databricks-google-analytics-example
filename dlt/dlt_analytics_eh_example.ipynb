{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c5ef99e-4039-4336-af6f-828537e486cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Delta Live Tables - Ingest Event Hub Messages from Google Analytics\n",
    "\n",
    "This sample DLT notebook code is designed to be used in conjunction with the Python producer code of the example repo (https://github.com/dublindata/databricks-google-analytics-example). Note that you will have to provide secret values for your event hub name and event hub connection string for this example to work. All schemas are predefined for you, but if you change the fields that you query from the Analytics API, you will need to modify this schema to get the values to show up.\n",
    "\n",
    "This notebook will create one Live table that represents the \"raw\" messages from event hub; each additional table builds off of it to show the acutal data in the payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4930aaef-a429-48d7-8849-fe2ac4c49d78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Event Hubs configuration\n",
    "# These are secret values, that should come from a secret scope. Change as appropriate.\n",
    "EH_CONN_STR = dbutils.secrets.get(scope=\"scope\", key=\"eh-conn-str\")\n",
    "EH_NAME = dbutils.secrets.get(scope=\"scope\", key=\"eh-name\")\n",
    "\n",
    "# Kafka Consumer configuration\n",
    "KAFKA_OPTIONS = {\n",
    "  \"kafka.bootstrap.servers\"  : f\"oneenv-eventhub.servicebus.windows.net:9093\",\n",
    "  \"subscribe\"                : f\"{EH_NAME}\",\n",
    "  \"kafka.sasl.mechanism\"     : \"PLAIN\",\n",
    "  \"kafka.security.protocol\"  : \"SASL_SSL\",\n",
    "  \"kafka.sasl.jaas.config\"   : f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"{EH_CONN_STR}\\\";\",\n",
    "  \"failOnDataLoss\"           : \"false\"\n",
    "}\n",
    "\n",
    "# PAYLOAD SCHEMA\n",
    "payload_ddl = \"messageType INT, data STRING\"\n",
    "payload_ddl_type0 = \"pageTitle STRING, screenPageViews STRING, asOf STRING\"\n",
    "payload_ddl_type1 = \"eventName STRING, eventCount STRING, asOf STRING\"\n",
    "payload_ddl_type2 = \"unifiedScreenName STRING, screenPageViews STRING, asOf STRING\"\n",
    "\n",
    "payload_schema = T._parse_datatype_string(payload_ddl)\n",
    "payload_schema_type0 = T._parse_datatype_string(payload_ddl_type0)\n",
    "payload_schema_type1 = T._parse_datatype_string(payload_ddl_type1)\n",
    "payload_schema_type2 = T._parse_datatype_string(payload_ddl_type2)\n",
    "\n",
    "\n",
    "# Basic record parsing and adding ETL audit columns\n",
    "def parse(df,schema):\n",
    "  return (df\n",
    "    .withColumn(\"records\", col(\"value\").cast(\"string\"))\n",
    "    .withColumn(\"parsed_records\", from_json(col(\"records\"), schema))\n",
    "    .withColumn(\"eh_enqueued_timestamp\", expr(\"timestamp\"))\n",
    "    .withColumn(\"eh_enqueued_date\", expr(\"to_date(timestamp)\"))\n",
    "    .withColumn(\"etl_processed_timestamp\", col(\"current_timestamp\"))\n",
    "    .withColumn(\"etl_rec_uuid\", expr(\"uuid()\"))\n",
    "    .drop(\"records\", \"value\", \"key\")\n",
    "  )\n",
    "\n",
    "@dlt.create_table(\n",
    "  comment=\"Raw Messages From Event Hub\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\",\n",
    "    \"pipelines.reset.allowed\": \"false\" # preserves the data in the delta table if you do full refresh\n",
    "  }\n",
    ")\n",
    "\n",
    "def messages_eventhub_raw():\n",
    "  return (\n",
    "   spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    "    .transform(parse, payload_schema)\n",
    "  )\n",
    "\n",
    "@dlt.create_table(\n",
    "  comment=\"Page Views by Page Title\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\",\n",
    "    \"pipelines.reset.allowed\": \"false\" # preserves the data in the delta table if you do full refresh\n",
    "  }    \n",
    ")\n",
    "def bronze_eh_page_count_by_page_name():\n",
    "  df = spark.table(\"messages_eventhub_raw\")\n",
    "  return(\n",
    "    dlt.read(\"messages_eventhub_raw\")\n",
    "      .filter(col('parsed_records.messageType') == 0)\n",
    "      .withColumn(\"page_view_data\", from_json(col(\"parsed_records.data\"), payload_schema_type0))\n",
    "      .select(\"page_view_data.pageTitle\",\"page_view_data.screenPageViews\",\"page_view_data.asOf\")\n",
    "  )\n",
    "\n",
    "\n",
    "@dlt.create_table(\n",
    "  comment=\"Event Counts\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\",\n",
    "    \"pipelines.reset.allowed\": \"false\" # preserves the data in the delta table if you do full refresh\n",
    "  }    \n",
    ")\n",
    "def bronze_event_count():\n",
    "  df = spark.table(\"messages_eventhub_raw\")\n",
    "  return(\n",
    "    dlt.read(\"messages_eventhub_raw\")\n",
    "      .filter(col('parsed_records.messageType') == 1)\n",
    "      .withColumn(\"event_data\", from_json(col(\"parsed_records.data\"), payload_schema_type1))\n",
    "      .select(\"event_data.eventName\",\"event_data.eventCount\",\"event_data.asOf\")\n",
    "  )\n",
    "\n",
    "\n",
    "@dlt.create_table(\n",
    "  comment=\"Views By Page\",\n",
    "  table_properties={\n",
    "    \"quality\": \"silver\",\n",
    "    \"pipelines.reset.allowed\": \"false\" # preserves the data in the delta table if you do full refresh\n",
    "  }    \n",
    ")\n",
    "def bronze_views_by_page():\n",
    "  df = spark.table(\"messages_eventhub_raw\")\n",
    "  return(\n",
    "    dlt.read(\"messages_eventhub_raw\")\n",
    "      .filter(col('parsed_records.messageType') == 2)\n",
    "      .withColumn(\"page_views\", from_json(col(\"parsed_records.data\"), payload_schema_type2))\n",
    "      .select(\"page_views.unifiedScreenName\",\"page_views.screenPageViews\",\"page_views.asOf\")\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DLT - Event Hub & Kafka",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
