{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0233bda-c512-4775-acfe-79d5b1d47ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Google Anaytics - Autoloader Example\n",
    "\n",
    "This example takes raw messages written to Azure storage from the sameple Python producer () and reads data from each folder that corresponds to a different message type.\n",
    "\n",
    "Note: You will need to either configure an external location in your Unity Catalog that matches your storage location (```abfss://```) or provide a storge account key as part of your DLT configuration for the pipelines to be able to read your data. Also, please set your storage account and container names on lines 3 and 4.\n",
    "\n",
    "This code will create 3 live tables for all valid message types. Note that if you don't have this many types, you will need to add/remove some configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d6e2718-b9b7-4e46-9834-1f9e58855d21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "storage_container = \"<storage_container_name>\"\n",
    "storage_account_name = \"<strorage_account_name>\"\n",
    "\n",
    "@dlt.create_table(\n",
    "  comment=\"Raw Type 0 Messages From Storage Account\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\",\n",
    "    \"pipelines.reset.allowed\": \"false\" # preserves the data in the delta table if you do full refresh\n",
    "  }\n",
    ")\n",
    "def messages_type_0_raw():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"json\")\n",
    "      .load(f'abfss://{storage_container}@{stroage_account_name}.dfs.core.windows.net/0')\n",
    "  )\n",
    "\n",
    "@dlt.create_table(\n",
    "  comment=\"Raw Type 1 Messages From Storage Account\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\",\n",
    "    \"pipelines.reset.allowed\": \"false\" # preserves the data in the delta table if you do full refresh\n",
    "  }\n",
    ")\n",
    "def messages_type_1_raw():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"json\")\n",
    "      .load(f'abfss://{storage_container}@{stroage_account_name}.dfs.core.windows.net/1')\n",
    "  )\n",
    "\n",
    "\n",
    "@dlt.create_table(\n",
    "  comment=\"Raw Type 2 Messages From Storage Account\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\",\n",
    "    \"pipelines.reset.allowed\": \"false\" # preserves the data in the delta table if you do full refresh\n",
    "  }\n",
    ")\n",
    "def messages_type_2_raw():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"json\")\n",
    "      .load(f'abfss://{storage_container}@{stroage_account_name}.dfs.core.windows.net/2')\n",
    "  )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DLT - Autoloader From Storage Account",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
